{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"B Mitra\" size=4>\n",
    "<div dir=rtl align=center>\n",
    "<br>\n",
    "<img src=\"https://aut.ac.ir/templates/tmpl_modern01/images/logo_fa.png\" alt=\"Amirkabir University Logo\" width=\"100\">\n",
    "<br>\n",
    "<font size=6>\n",
    "<b>پروژه دوم داده کاوی (بخش دوم)</b>\n",
    "<br>\n",
    "<font size=5> استاد درس: دکتر فاطمه شاکری\n",
    "<br>\n",
    "<font size=5> طراحان پروژه: مهدی رجالی، مهدی غیاثی، سینا ارزبین\n",
    "<br>\n",
    "<font size=4> پاییز ۱۴۰۳\n",
    "<hr>\n",
    "</div>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"B Nazanin\">\n",
    "<div dir=rtl>\n",
    "<p align=\"justify\">\n",
    "<font size=5><font color='#eb4034'>\n",
    "توجه داشته باشید علاوه بر پیاده سازی خواسته شده، توضیحات لازم در محل تعریف شده را بیان کنید.\n",
    "</p>\n",
    "</font>\n",
    "</div>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<h2>\n",
    "<font face=\"B Nazanin\" color=\"#0099cc\">\n",
    "بارگذاری کتابخانه های مورد نیاز"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<h2>\n",
    "<font face=\"B Nazanin\" color=\"#0099cc\">\n",
    "بارگذاری مجموعه داده"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('GDSC Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<h2>\n",
    "<font face=\"B Nazanin\" color=\"#0099cc\">\n",
    "جداسازی مجموعه داده‌های Test ،Train و Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<font face=\"B Nazanin\" size=4>\n",
    "در این بخش می‌خواهیم جداسازی مجموعه داده‌ها را بر حسب ویژگی‌های موجود انجام دهیم.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<div dir=rtl>\n",
    "<h4>\n",
    "<font face=\"B Nazanin\">\n",
    "<b>1. جداسازی برحسب نوع بافت در هر دارو</b>\n",
    "</font>\n",
    "</h4>\n",
    "<div dir=rtl>\n",
    "<font face=\"B Nazanin\" size=4>\n",
    "تابعی بنویسید که از هر\n",
    "Drug\n",
    "تعداد یکسانی\n",
    "General tissue description\n",
    "را انتخاب کند و داده‌های مربوط به آن‌ها از مجموعه‌ی آموزشی جدا کرده و در مجموعه‌ی آزمایشی قرار دهد.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tissue_train_test_split(df_train, count, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    df_train = df_train.copy()\n",
    "    df_test = pd.DataFrame(columns=df_train.columns)\n",
    "\n",
    "    # گروه‌بندی داده‌ها بر اساس Drug و General tissue description\n",
    "    grouped = df_train.groupby(['Drug', 'General tissue description'])\n",
    "\n",
    "    for name, group in grouped:\n",
    "        # انتخاب تعداد مشخصی از نمونه‌ها به صورت تصادفی، یا همه نمونه‌ها اگر کمتر از count باشند\n",
    "        n_samples_to_take = min(len(group), count)\n",
    "        \n",
    "        if n_samples_to_take > 0:  # بررسی وجود نمونه‌ها\n",
    "            # انتخاب نمونه‌ها\n",
    "            test_samples = group.sample(n=n_samples_to_take, random_state=random_state)\n",
    "            \n",
    "            # افزودن نمونه‌های انتخاب شده به df_test\n",
    "            df_test = pd.concat([df_test, test_samples], ignore_index=True)\n",
    "            \n",
    "            # حذف نمونه‌های انتخاب شده از df_train\n",
    "            df_train = df_train.drop(test_samples.index)\n",
    "\n",
    "    return df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "# مثال استفاده\n",
    "# df_train1, df_test1 = tissue_train_test_split(df, count=3, random_state=42)\n",
    "# df_train1, df_val1 = tissue_train_test_split(df_train1, count=3, random_state=42)\n",
    "\n",
    "# نمایش اندازه مجموعه‌ها\n",
    "# print(\"Train set size:\", len(df_train1))\n",
    "# print(\"Validation set size:\", len(df_val1))\n",
    "# print(\"Test set size:\", len(df_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 209828\n",
      "Validation set size: 15991\n",
      "Test set size: 16216\n"
     ]
    }
   ],
   "source": [
    "df_train1, df_test1 = tissue_train_test_split(df, count=3, random_state=42)\n",
    "df_train1, df_val1 = tissue_train_test_split(df_train1, count=3, random_state=42)\n",
    "\n",
    "print(\"Train set size:\", len(df_train1))\n",
    "print(\"Validation set size:\", len(df_val1))\n",
    "print(\"Test set size:\", len(df_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<h4>\n",
    "<font face=\"B Nazanin\">\n",
    "<b>2. جداسازی برحسب هدف بیولوژیکی در هر مسیر بیولوژیکی</b>\n",
    "</font>\n",
    "</h4>\n",
    "<div dir=rtl>\n",
    "<font face=\"B Nazanin\" size=4>\n",
    "تابعی بنویسید که از هر\n",
    "Target pathway\n",
    "درصد یکسانی از مقادیر مختلف\n",
    "Biological target\n",
    "را انتخاب کند و داده‌های مربوط به آن‌ها از مجموعه‌ی آموزشی جدا کرده و در مجموعه‌ی آزمایشی قرار دهد.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def biological_target_train_test_split(df_train, portion, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    df_train = df_train.copy()\n",
    "    df_test = pd.DataFrame(columns=df_train.columns)\n",
    "\n",
    "    # گروه‌بندی داده‌ها بر اساس Target pathway و Biological target\n",
    "    grouped = df_train.groupby(['Target pathway', 'Biological target'])\n",
    "\n",
    "    for name, group in grouped:\n",
    "        # محاسبه تعداد نمونه‌ها برای مجموعه آزمایشی\n",
    "        n_test_samples = int(len(group) * portion)\n",
    "        \n",
    "        # انتخاب تصادفی نمونه‌ها\n",
    "        test_samples = group.sample(n=n_test_samples, random_state=random_state)\n",
    "        \n",
    "        # افزودن نمونه‌های انتخاب شده به df_test\n",
    "        df_test = pd.concat([df_test, test_samples])\n",
    "        \n",
    "        # حذف نمونه‌های انتخاب شده از df_train\n",
    "        df_train = df_train.drop(test_samples.index)\n",
    "\n",
    "    return df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "# مثال استفاده\n",
    "# df_train: DataFrame شامل داده‌های اولیه\n",
    "# df_train, df_test = biological_target_train_test_split(df_train, 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 164821\n",
      "Validation set size: 34319\n",
      "Test set size: 42895\n"
     ]
    }
   ],
   "source": [
    "df_train2, df_test2 = biological_target_train_test_split(df, portion=0.2, random_state=42)\n",
    "df_train2, df_val2 = biological_target_train_test_split(df_train2, portion=0.2, random_state=42)\n",
    "\n",
    "print(\"Train set size:\", len(df_train2))\n",
    "print(\"Validation set size:\", len(df_val2))\n",
    "print(\"Test set size:\", len(df_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<h2>\n",
    "<font face=\"B Nazanin\" color=\"#0099cc\">\n",
    "مدیریت مقادیر گم‌شده"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<font face=\"B Nazanin\" size=4>\n",
    "از تابعی که در بخش اول پروژه پیاده سازی کرده‌اید استفاده کنید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputation_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mode(group, column, calculate_mode, default_val=np.nan):\n",
    "    if calculate_mode:\n",
    "        mode_val = group.mode()\n",
    "        train_imputation_data[f'{group.name}_{column}'] = mode_val[0] if not mode_val.empty else default_val\n",
    "    \n",
    "    return train_imputation_data.get(f'{group.name}_{column}', default_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df: pd.DataFrame, train_set: bool):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Imputation of 'Cancer Type' column\n",
    "    df['Cancer Type'] = df.groupby(by=['Drug', 'Specific tissue description'], dropna=False)['Cancer Type'].transform(\n",
    "                            lambda group: group.fillna(get_mode(group, column='Cancer Type', calculate_mode=train_set))\n",
    "                        )\n",
    "\n",
    "    df['Cancer Type'] = df.groupby(by=['Drug', 'General tissue description'], dropna=False)['Cancer Type'].transform(\n",
    "                            lambda group: group.fillna(get_mode(group, column='Cancer Type', calculate_mode=train_set))\n",
    "                        )\n",
    "    \n",
    "\n",
    "    # Imputation of 'Specific tissue description' and 'General tissue description' column\n",
    "    columns_to_fill = ['General tissue description', 'Specific tissue description']\n",
    "    for col in columns_to_fill:\n",
    "        df[col] = df.groupby(by=['Drug', 'Cancer Type'], dropna=False)[col].transform(\n",
    "                                lambda group: group.fillna(get_mode(group, column=col, calculate_mode=train_set))\n",
    "                            )\n",
    "        \n",
    "    # Imputation of 'Biological target' and 'Target pathway' column\n",
    "    columns_to_fill = ['Biological target', 'Target pathway']\n",
    "    for col in columns_to_fill:\n",
    "        df[col] = df.groupby(by=['Drug'], dropna=False)[col].transform(\n",
    "                                lambda group: group.fillna(get_mode(group, column=col, calculate_mode=train_set))\n",
    "                            )\n",
    "        \n",
    "    # Imputation of ['Microsatellite instability Status', 'Screen Medium', 'Growth Properties', 'CNA', 'Gene Expression', 'Methylation'] column\n",
    "    columns_to_fill = ['Microsatellite instability Status', 'Screen Medium', 'Growth Properties', 'CNA', 'Gene Expression', 'Methylation']\n",
    "    for col in columns_to_fill:\n",
    "        df[col] = df.groupby(by=['Drug', 'General tissue description'], dropna=False)[col].transform(\n",
    "                                lambda group: group.fillna(get_mode(group, column=col, calculate_mode=train_set))\n",
    "                            )\n",
    "        \n",
    "    \n",
    "    df.fillna('Unknown', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1 = impute_missing_values(df=df_train1, train_set=True)\n",
    "df_val1 = impute_missing_values(df=df_val1, train_set=False)\n",
    "df_test1 = impute_missing_values(df=df_test1, train_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2 = impute_missing_values(df=df_train2, train_set=True)\n",
    "df_val2 = impute_missing_values(df=df_val2, train_set=False)\n",
    "df_test2 = impute_missing_values(df=df_test2, train_set=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<h2>\n",
    "<font face=\"B Nazanin\" color=\"#0099cc\">\n",
    "آموزش مدل:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<h4>\n",
    "<font face=\"B Nazanin\">\n",
    "<b>تفکیک ویژگی‌ها و برچسب‌ها</b>\n",
    "</font>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_train1.iloc[:, :-1] \n",
    "target = df_train1.iloc[:, -1] \n",
    "\n",
    "X_train1 = features\n",
    "Y_train1 = target\n",
    "\n",
    "X_test1 = df_test1.iloc[:, :-1]\n",
    "Y_test1 = df_test1.iloc[:, -1]\n",
    "\n",
    "X_valid1 = df_val1.iloc[:, :-1]\n",
    "Y_valid1 = df_val1.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Also for validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = df_train2.iloc[:, :-1]\n",
    "Y_train2 = target = df_train2.iloc[:, -1] \n",
    "\n",
    "X_test2 = df_test2.iloc[:, :-1]\n",
    "Y_test2 = df_test2.iloc[:, -1]\n",
    "\n",
    "X_valid2 = df_val2.iloc[:, :-1]\n",
    "Y_valid2 = df_val2.iloc[:, -1]\n",
    "# Also for validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<h4>\n",
    "<font face=\"B Nazanin\">\n",
    "<b>1. مدل CatBoost با جداسازی اول</b>\n",
    "</font>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'Cell line identifier', 'Drug', 'General tissue description',\n",
    "    'Specific tissue description', 'Cancer Type', \n",
    "    'Microsatellite instability Status', 'Screen Medium', \n",
    "    'Growth Properties', 'CNA', 'Gene Expression', \n",
    "    'Methylation', 'Biological target', 'Target pathway'\n",
    "]\n",
    "train_pool= Pool(data=X_train1, label=Y_train1, cat_features=categorical_features)\n",
    "val_pool = Pool(data=X_valid1, label=Y_valid1,cat_features=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.6775368\ttest: 2.6673836\tbest: 2.6673836 (0)\ttotal: 131ms\tremaining: 1m 5s\n",
      "50:\tlearn: 1.4372061\ttest: 1.4225192\tbest: 1.4225192 (50)\ttotal: 6.68s\tremaining: 58.8s\n",
      "100:\tlearn: 1.3256074\ttest: 1.2988118\tbest: 1.2988118 (100)\ttotal: 13.1s\tremaining: 51.7s\n",
      "150:\tlearn: 1.2845064\ttest: 1.2563081\tbest: 1.2563081 (150)\ttotal: 19.6s\tremaining: 45.4s\n",
      "200:\tlearn: 1.2619673\ttest: 1.2331108\tbest: 1.2331108 (200)\ttotal: 26.2s\tremaining: 39s\n",
      "250:\tlearn: 1.2451914\ttest: 1.2145906\tbest: 1.2145906 (250)\ttotal: 33.3s\tremaining: 33.1s\n",
      "300:\tlearn: 1.2334617\ttest: 1.2035207\tbest: 1.2035207 (300)\ttotal: 39.8s\tremaining: 26.3s\n",
      "350:\tlearn: 1.2246255\ttest: 1.1954764\tbest: 1.1954764 (350)\ttotal: 46.6s\tremaining: 19.8s\n",
      "400:\tlearn: 1.2151627\ttest: 1.1858272\tbest: 1.1858272 (400)\ttotal: 53.1s\tremaining: 13.1s\n",
      "450:\tlearn: 1.2088406\ttest: 1.1800379\tbest: 1.1800379 (450)\ttotal: 59.8s\tremaining: 6.49s\n",
      "499:\tlearn: 1.2036960\ttest: 1.1751825\tbest: 1.1751825 (499)\ttotal: 1m 6s\tremaining: 0us\n",
      "bestTest = 1.175182515\n",
      "bestIteration = 499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x209f3328550>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = CatBoostRegressor(\n",
    "    \n",
    "    iterations=500,           \n",
    "    learning_rate=0.05,       \n",
    "    depth=6,                  \n",
    "    loss_function='RMSE',     \n",
    "    random_seed=42,           \n",
    "    verbose=50,\n",
    "    task_type=\"GPU\", # Uncomment for faster training if a dedicated GPU is available\n",
    "    devices='0',\n",
    "    cat_features=categorical_features\n",
    "   \n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    train_pool,               \n",
    "    eval_set=val_pool,        \n",
    "    early_stopping_rounds=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "train_predictions1 = model1.predict(train_pool)\n",
    "val_predictions1 = model1.predict(val_pool)\n",
    "test_predictions1 = model1.predict(X_test1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<h4>\n",
    "<font face=\"B Nazanin\">\n",
    "<b>2. مدل CatBoost با جداسازی دوم</b>\n",
    "</font>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'Cell line identifier', 'Drug', 'General tissue description',\n",
    "    'Specific tissue description', 'Cancer Type', \n",
    "    'Microsatellite instability Status', 'Screen Medium', \n",
    "    'Growth Properties', 'CNA', 'Gene Expression', \n",
    "    'Methylation', 'Biological target', 'Target pathway'\n",
    "]\n",
    "train_pool= Pool(data=X_train2, label=Y_train2, cat_features=categorical_features)\n",
    "val_pool = Pool(data=X_valid2, label=Y_valid2,cat_features=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.6408339\ttest: 2.7423284\tbest: 2.7423284 (0)\ttotal: 103ms\tremaining: 51.2s\n",
      "50:\tlearn: 1.4264389\ttest: 1.4243224\tbest: 1.4243224 (50)\ttotal: 5.16s\tremaining: 45.5s\n",
      "100:\tlearn: 1.3267956\ttest: 1.3030025\tbest: 1.3030025 (100)\ttotal: 10.2s\tremaining: 40.4s\n",
      "150:\tlearn: 1.2942269\ttest: 1.2613369\tbest: 1.2613369 (150)\ttotal: 15.4s\tremaining: 35.5s\n",
      "200:\tlearn: 1.2673949\ttest: 1.2271068\tbest: 1.2271068 (200)\ttotal: 20.6s\tremaining: 30.6s\n",
      "250:\tlearn: 1.2497036\ttest: 1.2057585\tbest: 1.2057585 (250)\ttotal: 25.8s\tremaining: 25.6s\n",
      "300:\tlearn: 1.2392940\ttest: 1.1938742\tbest: 1.1938742 (300)\ttotal: 31.2s\tremaining: 20.6s\n",
      "350:\tlearn: 1.2294609\ttest: 1.1826493\tbest: 1.1826493 (350)\ttotal: 36.3s\tremaining: 15.4s\n",
      "400:\tlearn: 1.2205597\ttest: 1.1724761\tbest: 1.1724761 (400)\ttotal: 41.5s\tremaining: 10.3s\n",
      "450:\tlearn: 1.2132844\ttest: 1.1653590\tbest: 1.1653590 (450)\ttotal: 47.1s\tremaining: 5.12s\n",
      "499:\tlearn: 1.2073077\ttest: 1.1592885\tbest: 1.1592885 (499)\ttotal: 52.1s\tremaining: 0us\n",
      "bestTest = 1.159288519\n",
      "bestIteration = 499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x209f0869ac0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = CatBoostRegressor(\n",
    "    iterations=500,           \n",
    "    learning_rate=0.05,       \n",
    "    depth=6,                  \n",
    "    loss_function='RMSE',     \n",
    "    random_seed=42,           \n",
    "    verbose=50,\n",
    "    task_type=\"GPU\", # Uncomment for faster training if a dedicated GPU is available\n",
    "    devices='0',\n",
    "    cat_features=categorical_features\n",
    "   \n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    train_pool,               \n",
    "    eval_set=val_pool,        \n",
    "    early_stopping_rounds=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "train_predictions2 = model2.predict(train_pool)\n",
    "val_predictions2 = model2.predict(val_pool)\n",
    "test_predictions2 = model2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<h2>\n",
    "<font face=\"B Nazanin\" color=\"#0099cc\">\n",
    "ارزیابی مدل‌ها:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<font face=\"B Nazanin\" size=4>\n",
    "<code>R2 Score</code> را برای هر مدل و بر روی هر کدام از مجموع داده‌ها محاسبه کنید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost - Train - R2 Score: 0.8448\n",
      "CatBoost - Validation - R2 Score: 0.8179\n",
      "CatBoost - Test - R2 Score: 0.8128\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(y_true, y_pred, dataset_name):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{dataset_name} - R2 Score: {r2:.4f}\")\n",
    "\n",
    "evaluate_model(Y_train1, train_predictions1, \"CatBoost - Train\")\n",
    "evaluate_model(Y_valid1, val_predictions1, \"CatBoost - Validation\")\n",
    "evaluate_model(Y_test1, test_predictions1, \"CatBoost - Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost - Train - R2 Score: 0.8368\n",
      "CatBoost - Validation - R2 Score: 0.8326\n",
      "CatBoost - Test - R2 Score: 0.8319\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(y_true, y_pred, dataset_name):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{dataset_name} - R2 Score: {r2:.4f}\")\n",
    "\n",
    "evaluate_model(Y_train2, train_predictions2, \"CatBoost - Train\")\n",
    "evaluate_model(Y_valid2, val_predictions2, \"CatBoost - Validation\")\n",
    "evaluate_model(Y_test2, test_predictions2, \"CatBoost - Test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<font face=\"B Nazanin\" size=4>\n",
    "نتایج به دست آمده را بر اساس Feature Importance‌های بدست‌آمده در بخش قبل توجیه کنید.\n",
    "به نظر شما دلیل این کاهش بسیار شدید در کارایی مدل با وجود ویژگی‌های پراهمیت‌تری که هنوز موجود هستند چیست؟\n",
    "\n",
    "به نظرتان برای پیش‌بینی دارو‌های جدید و یا دارو‌هایی که روی نمونه‌های سلولی ناآشنا (Cell Line‌های جدید) در آزمایشگاه بررسی شده‌اند، \n",
    "می‌توان مدلی با کارایی خوب با ویژگی‌های موجود ارائه داد؟\n",
    "\n",
    "همچنین به نظر شما بین دو مدل CatBoost و Random Forest آموزش داده‌شده در بخش قبل، کدام یک در رویارویی با نمونه‌هایی با دارو‌های ناشناخته یا هدف‌های زیستی جدید مناسب خواهد بود؟ چرا؟\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"B Nazanin\">\n",
    "<div dir=rtl>\n",
    "<p align=\"justify\">\n",
    "<font size=4>\n",
    "<b> <font color='#eb4034'> توضیحات پاسخ: </font></b>  لطفا اینجا بنویسید\n",
    "</p>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<h2>\n",
    "<font face=\"B Nazanin\" color=\"#FF8F00\">\n",
    "سوال امتیازی"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl>\n",
    "<font face=\"B Nazanin\" size=4>\n",
    "یکی از مشکلات اساسی که برای شیوهٔ رمز‌گذاری One Hot مطرح کردیم، افزایش تعداد ویژگی‌ها و پربعد کردن داده‌ها بود.\n",
    "\n",
    "ما در گذشته یک شیوه برای کاهش بعد داده‌ها مطالعه کردیم که الگوریتم PCA بود.\n",
    "سوالی که ممکن است پیش بیاید این است که چرا با روش One Hot رمزگذاری نکنیم و سپس با PCA با افزایش بعد ناشی از این رمز‌گذاری مبارزه کنیم؟\n",
    "\n",
    "این کار، یعنی استفاده از PCA پس از رمز‌گذاری داده‌ها به شیوهٔ One Hot، به چند دلیل صحیح نیست.\n",
    "در مورد آن مطالعه کنید و با استدلال هر یک از دلایل خود را توضیح دهید.\n",
    "\n",
    "دقت کنید که دلایل مد نظر مرتبط با مطالبی هستند که شما مطالعه کرده‌اید، و دلایلی مانند پایداری محاسباتی و ... مد نظر نیستند.\n",
    "</font>\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
